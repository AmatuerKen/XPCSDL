{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab53c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset, random_split, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torchmetrics import MeanSquaredError\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from functions_dataset import *\n",
    "#from functions_training import train_with_scheduler_and_early_stopping_memory_monitor\n",
    "from functions_bulk_runner import CorrZarrDataset\n",
    "from functions_finetuning import *\n",
    "\n",
    "from optuna.samplers import GridSampler\n",
    "from torch.amp import autocast, GradScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23731d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Ti\n",
      "BF16 supported: True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Check if BF16 is supported\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"BF16 supported: {torch.cuda.is_bf16_supported()}\")\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d9d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_max = 8\n",
    "\n",
    "binsize = 8\n",
    "bin = binsize\n",
    "\n",
    "directory = \"./step3_finetuning/\"\n",
    "\n",
    "shear_rate = 0.107\n",
    "tau0 = 0.0005\n",
    "\n",
    "dataset = CorrZarrDataset(\n",
    "    corr_zarr_path_features = directory + f\"corr_dataset_maxdt{dt_max:02d}_bin{bin:02d}_feature.zarr\",\n",
    "    corr_zarr_path_target = directory + f\"corr_dataset_maxdt{dt_max:02d}_bin{bin:02d}_target.npz\",\n",
    "    shear_rate=shear_rate * tau0 * 100 * 100,\n",
    ")\n",
    "\n",
    "idx_data = np.load(\"./step3_finetuning/dataset_split_index.npz\")\n",
    "idx_train = idx_data['idx_train']\n",
    "idx_val = idx_data['idx_val']\n",
    "idx_test = idx_data['idx_test']\n",
    "\n",
    "train_set = Subset(dataset, idx_train)\n",
    "val_set   = Subset(dataset, idx_val)\n",
    "test_set  = Subset(dataset, idx_test)\n",
    "#plot_target_distribution(train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378aabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_set, val_set, saving_memory = False):\n",
    "\n",
    "    n_conv = trial.suggest_int(\"n_conv\", 3, 9)\n",
    "    base_channels = trial.suggest_categorical(\"base_channels\", [4, 8, 16, 32, 64])\n",
    "    kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 5, 7])\n",
    "\n",
    "    #n_conv = 3\n",
    "    #base_channels = 4\n",
    "    #kernel_size = 3\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "\n",
    "    model = ConvRegressor(\n",
    "            n_conv=n_conv,\n",
    "            base_channels=base_channels,\n",
    "            kernel_size=kernel_size,\n",
    "        ).to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"Total parameters: {total_params/1000000:.2f} M\")   \n",
    "    print(f\"Parameter memory (GB): {total_params * 4 / 1024**3:.2f} GB\")\n",
    "    #print(\"Parameter type:\", next(model.parameters()).dtype)\n",
    "    \n",
    "\n",
    "    print(\"before training\")\n",
    "    #print(torch.cuda.memory_summary())\n",
    "    print_memory(torch.cuda.memory_allocated())\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    #test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    metric = MeanSquaredError().to(device)\n",
    "\n",
    "    #best_validation_accuracy = np.inf #loss\n",
    "\n",
    "    n_epochs = 20\n",
    "\n",
    "    #for epoch in range(n_epochs):\n",
    "    #history, _ = train_with_scheduler_and_early_stopping(model, optimizer, criterion, metric, train_loader, valid_loader,\n",
    "    #                n_epochs, device, scheduler, show_progress=False)\n",
    "\n",
    "    if saving_memory:\n",
    "        history, _ = train_with_scheduler_bestaccurary_memory_monitor_mixedprecision(model, optimizer, criterion, metric, train_loader, valid_loader,\n",
    "                        n_epochs, device, scheduler, show_progress=False, show_memory= True)\n",
    "    else:\n",
    "        history, _ = train_with_scheduler_bestaccurary_memory_monitor(model, optimizer, criterion, metric, train_loader, valid_loader,\n",
    "                        n_epochs, device, scheduler, show_progress=False, show_memory= True)\n",
    "\n",
    "    validation_accuracy = min(history[\"valid_metrics\"])\n",
    "\n",
    "        #if validation_accuracy < best_validation_accuracy:\n",
    "        #        best_validation_accuracy = validation_accuracy\n",
    "        #trial.report(validation_accuracy, epoch)\n",
    "        #if trial.should_prune():\n",
    "        #    raise optuna.TrialPruned()\n",
    "\n",
    "    return validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9976691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_with_data = lambda trial: objective(\n",
    "    trial, train_set=train_set, val_set=val_set, saving_memory=True)\n",
    "\n",
    "objective_with_data = partial(objective, train_set=train_set, val_set=val_set, saving_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9813a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-22 19:47:38,070]\u001b[0m A new study created in memory with name: no-name-200ab69f-6e95-4605-a983-d1397ff9cde9\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 0.07 M\n",
      "Parameter memory (GB): 0.00 GB\n",
      "before training\n",
      "2510.13 MB\n",
      "after forward, memory allocated\n",
      "2552.62 MB\n",
      "max memory allocated\n",
      "2575.39 MB\n",
      "after backward\n",
      "2510.83 MB\n",
      "max memory allocated\n",
      "2609.23 MB\n",
      "after optimizer step\n",
      "2511.08 MB\n",
      "max memory allocated\n",
      "2609.23 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-22 19:56:47,855]\u001b[0m Trial 0 finished with value: 0.1523773968219757 and parameters: {'n_conv': 3, 'base_channels': 16, 'kernel_size': 5, 'batch_size': 32}. Best is trial 0 with value: 0.1523773968219757.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 0.13 M\n",
      "Parameter memory (GB): 0.00 GB\n",
      "before training\n",
      "2510.37 MB\n",
      "after forward, memory allocated\n",
      "2519.16 MB\n",
      "max memory allocated\n",
      "2526.68 MB\n",
      "after backward\n",
      "2510.97 MB\n",
      "max memory allocated\n",
      "2548.08 MB\n",
      "after optimizer step\n",
      "2511.46 MB\n",
      "max memory allocated\n",
      "2548.08 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-22 20:09:15,787]\u001b[0m Trial 1 finished with value: 0.1545880287885666 and parameters: {'n_conv': 3, 'base_channels': 16, 'kernel_size': 7, 'batch_size': 8}. Best is trial 0 with value: 0.1523773968219757.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 0.07 M\n",
      "Parameter memory (GB): 0.00 GB\n",
      "before training\n",
      "2510.13 MB\n",
      "after forward, memory allocated\n",
      "2521.12 MB\n",
      "max memory allocated\n",
      "2530.07 MB\n",
      "after backward\n",
      "2510.50 MB\n",
      "max memory allocated\n",
      "2543.62 MB\n",
      "after optimizer step\n",
      "2510.76 MB\n",
      "max memory allocated\n",
      "2543.62 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-22 20:21:46,922]\u001b[0m Trial 2 finished with value: 0.15606029331684113 and parameters: {'n_conv': 3, 'base_channels': 16, 'kernel_size': 5, 'batch_size': 8}. Best is trial 0 with value: 0.1523773968219757.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 0.07 M\n",
      "Parameter memory (GB): 0.00 GB\n",
      "before training\n",
      "2510.14 MB\n",
      "after forward, memory allocated\n",
      "2531.17 MB\n",
      "max memory allocated\n",
      "2542.55 MB\n",
      "after backward\n",
      "2510.61 MB\n",
      "max memory allocated\n",
      "2564.57 MB\n",
      "after optimizer step\n",
      "2510.87 MB\n",
      "max memory allocated\n",
      "2564.57 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-22 20:32:27,489]\u001b[0m Trial 3 finished with value: 0.5189695954322815 and parameters: {'n_conv': 3, 'base_channels': 16, 'kernel_size': 5, 'batch_size': 16}. Best is trial 0 with value: 0.1523773968219757.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 0.13 M\n",
      "Parameter memory (GB): 0.00 GB\n",
      "before training\n",
      "2510.38 MB\n",
      "after forward, memory allocated\n",
      "2545.52 MB\n",
      "max memory allocated\n",
      "2563.50 MB\n",
      "after backward\n",
      "2511.31 MB\n",
      "max memory allocated\n",
      "2603.78 MB\n",
      "after optimizer step\n",
      "2511.80 MB\n",
      "max memory allocated\n",
      "2603.78 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-22 20:42:16,503]\u001b[0m Trial 4 finished with value: 0.1961766928434372 and parameters: {'n_conv': 3, 'base_channels': 16, 'kernel_size': 7, 'batch_size': 32}. Best is trial 0 with value: 0.1523773968219757.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 0.02 M\n",
      "Parameter memory (GB): 0.00 GB\n",
      "before training\n",
      "2509.98 MB\n",
      "after forward, memory allocated\n",
      "2559.66 MB\n",
      "max memory allocated\n",
      "2587.77 MB\n",
      "after backward\n",
      "2510.52 MB\n",
      "max memory allocated\n",
      "2619.79 MB\n",
      "after optimizer step\n",
      "2510.62 MB\n",
      "max memory allocated\n",
      "2619.79 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-22 20:51:10,401]\u001b[0m Trial 5 finished with value: 0.181216761469841 and parameters: {'n_conv': 3, 'base_channels': 16, 'kernel_size': 3, 'batch_size': 32}. Best is trial 0 with value: 0.1523773968219757.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 0.02 M\n",
      "Parameter memory (GB): 0.00 GB\n",
      "before training\n",
      "2509.99 MB\n",
      "after forward, memory allocated\n",
      "2534.83 MB\n",
      "max memory allocated\n",
      "2548.88 MB\n",
      "after backward\n",
      "2510.31 MB\n",
      "max memory allocated\n",
      "2566.47 MB\n",
      "after optimizer step\n",
      "2510.40 MB\n",
      "max memory allocated\n",
      "2566.47 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-22 21:02:59,899]\u001b[0m Trial 6 finished with value: 0.15833014249801636 and parameters: {'n_conv': 3, 'base_channels': 16, 'kernel_size': 3, 'batch_size': 16}. Best is trial 0 with value: 0.1523773968219757.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 0.02 M\n",
      "Parameter memory (GB): 0.00 GB\n",
      "before training\n",
      "2509.99 MB\n",
      "after forward, memory allocated\n",
      "2522.83 MB\n",
      "max memory allocated\n",
      "2533.02 MB\n",
      "after backward\n",
      "2510.20 MB\n",
      "max memory allocated\n",
      "2540.48 MB\n",
      "after optimizer step\n",
      "2510.30 MB\n",
      "max memory allocated\n",
      "2540.48 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-22 21:16:34,139]\u001b[0m Trial 7 finished with value: 1.5026805400848389 and parameters: {'n_conv': 3, 'base_channels': 16, 'kernel_size': 3, 'batch_size': 8}. Best is trial 0 with value: 0.1523773968219757.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 0.13 M\n",
      "Parameter memory (GB): 0.00 GB\n",
      "before training\n",
      "2510.39 MB\n",
      "after forward, memory allocated\n",
      "2527.96 MB\n",
      "max memory allocated\n",
      "2542.62 MB\n",
      "after backward\n",
      "2511.10 MB\n",
      "max memory allocated\n",
      "2566.66 MB\n",
      "after optimizer step\n",
      "2511.59 MB\n",
      "max memory allocated\n",
      "2566.66 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-22 21:26:52,167]\u001b[0m Trial 8 finished with value: 0.15032020211219788 and parameters: {'n_conv': 3, 'base_channels': 16, 'kernel_size': 7, 'batch_size': 16}. Best is trial 8 with value: 0.15032020211219788.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "{'n_conv': 3, 'base_channels': 16, 'kernel_size': 7, 'batch_size': 16}\n",
      "Best validation loss: 0.15032020211219788\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "search_space = {\n",
    "    \"n_conv\": [3],\n",
    "    \"base_channels\": [16],\n",
    "    \"kernel_size\": [3, 5, 7],\n",
    "    \"batch_size\": [8, 16, 32],\n",
    "}\n",
    "\n",
    "sampler = GridSampler(search_space)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "study.optimize(objective_with_data)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_params)\n",
    "\n",
    "print(\"Best validation loss:\", study.best_value)\n",
    "np.savez('./step3_finetuning/optuna_best_study_10.npz', study = study)\n",
    "\n",
    "#3, 4 ...1\n",
    "#4, 4 ...2\n",
    "#5, 4 ...3\n",
    "#6, 4 ...4\n",
    "#7, 4 ...5\n",
    "#8, 4 ...6\n",
    "#3, 8 ...7\n",
    "#4, 8 ...8\n",
    "#5-8, 8 ...9\n",
    "#can't run 9 layers locally\n",
    "#3, 16 ...10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0166ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('./step3_finetuning/optuna_best_study_7.npz', study = study)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XPCS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
